{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple 3-gram Language Modeling\n",
    "\n",
    "The sentence start character is `<s>` while the end character is `</s>`. As it is a 3-gram model, it needs to pad 2 `<s>` at the beginning of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, time\n",
    "import itertools, collections\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import ngram_lm\n",
    "import ngram_utils\n",
    "from shared_lib import utils, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/lm.pkl', 'rb') as f:\n",
    "    lm = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(context, n):\n",
    "    if n > 50000:\n",
    "        print 'Exceed the vocabulary size 50000!'\n",
    "        return\n",
    "    \n",
    "    probs = [lm.next_word_proba(word, context) for word in lm.words]\n",
    "    combined = zip(lm.words, probs)\n",
    "    top_n = sorted(combined, key=lambda t: t[1], reverse=True)[:n]\n",
    "    return [word_prob[0].decode('utf-8') for word_prob in top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我\n",
      "你\n",
      "他\n",
      "但\n",
      "不\n"
     ]
    }
   ],
   "source": [
    "# Given the first 2 words\n",
    "context = [\"<s>\", \"<s>\"]\n",
    "result = get_top_n(context, 5)\n",
    "\n",
    "for word in result:\n",
    "    print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，\n",
      "。\n",
      "吗\n",
      "像\n",
      "？\n"
     ]
    }
   ],
   "source": [
    "# Given the first 2 words\n",
    "context = [\"你\", \"好\"]\n",
    "result = get_top_n(context, 5)\n",
    "\n",
    "for word in result:\n",
    "    print word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> 他 在 监 视 机 和 咖 啡 师 . . </s>\n",
      "[11 tokens; log P(seq): -55.13]\n",
      "\n",
      "<s> <s> 感 情 跟 踪 我 。 不 可 能 是 - 没 错 。 </s>\n",
      "[14 tokens; log P(seq): -74.85]\n",
      "\n",
      "<s> <s> 像 一 颗 星 球 最 快 增 长 。 </s>\n",
      "[10 tokens; log P(seq): -51.88]\n",
      "\n",
      "<s> <s> 在 报 告 上 说 道 ， 但 是 星 星 从 尤 文 ， 埃 伦 向 我 招\n",
      "[20 tokens; log P(seq): -120.26]\n",
      "\n",
      "<s> <s> 今 天 我 不 能 傻 坐 着 休 闲 和 工 作 ， </s>\n",
      "[14 tokens; log P(seq): -64.60]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\", \"<s>\"]\n",
    "    for i in range(max_length):\n",
    "        seq.append(ngram_utils.predict_next(lm, seq))\n",
    "        # Stop at end-of-sentence.\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print \" \".join(seq)\n",
    "    print \"[{1:d} tokens; log P(seq): {0:.02f}]\".format(*ngram_utils.score_seq(lm, seq))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and proprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.tokenizeChinese(\"data/valid.zh\", \"data/valid.tok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000,)\n"
     ]
    }
   ],
   "source": [
    "sent_arr = []\n",
    "for line in open(\"data/valid.tok\", \"r\").readlines():\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    sent_arr.append(words)\n",
    "    \n",
    "sentences = np.asarray(sent_arr)\n",
    "print sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sents_to_tokens(sents):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with padding for a trigram model.\"\"\"\n",
    "    padded_sentences = ([\"<s>\", \"<s>\"] + s + [\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([utils.canonicalize_word(w, wordset=lm.words) \n",
    "                     for w in utils.flatten(padded_sentences)], dtype=object)\n",
    "\n",
    "test_tokens = sents_to_tokens(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity: 39.26\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, test_tokens)\n",
    "print \"Test perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
